{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39272,"databundleVersionId":4629629,"sourceType":"competition"},{"sourceId":5058945,"sourceType":"datasetVersion","datasetId":2937417},{"sourceId":7461426,"sourceType":"datasetVersion","datasetId":4342699},{"sourceId":7497568,"sourceType":"datasetVersion","datasetId":4343680}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install seaborn imutils --quiet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math, random\nimport imutils\nimport os\nimport gc\nimport cv2\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_visuals = True\ntest_on_data = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv_path = \"/kaggle/input/rsna-breast-cancer-detection/train.csv\"\ntrain_image_dir = \"/kaggle/input/rsna-mammogram-pngs-enhanced/png_train_images\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(train_csv_path)\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_ = train[['patient_id', 'image_id', 'cancer','difficult_negative_case']]\nimages_['file_path'] = images_.apply(lambda x: os.path.join(train_image_dir, str(x.patient_id), str(x.image_id) + '.png'), axis=1)\nimages_.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The number and proportion of the positive and negative cases","metadata":{}},{"cell_type":"code","source":"pro = images_.groupby(\"cancer\")['difficult_negative_case'].value_counts().reset_index()\npro['proportion'] = pro['count'] / len(images_)\ndisplay(pro)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef has_cancer(c):\n    return c > 0\n\ncancer_per_patient = train.groupby(\"patient_id\")[\"cancer\"].sum().apply(has_cancer)\n\nif show_visuals:\n    ax = sns.countplot(x=cancer_per_patient)\n    ax.bar_label(ax.containers[0])\n\n    plt.xlabel(\"Cancer\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Number of patients with cancer\")\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The age distribution","metadata":{}},{"cell_type":"code","source":"# g = sns.FacetGrid(train, col = 'cancer')\n# g.map(sns.histplot, 'age' )\n\n## Alternative: detailed and bigger view\nif show_visuals:\n    sns.displot(train[train['cancer']==1].age, bins=50, kde=False)\n    sns.displot(train[(train['cancer']==0) & (train['difficult_negative_case']==True)].age, bins=50, kde=False)\n    sns.displot(train[(train['cancer']==0) & (train['difficult_negative_case']==False)].age, bins=50, kde=False)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As Cancer and not cancer are unbalanced interms of samples , we would have to sample data from both class so that model gets the general undrstanding of what both type of data looks like","metadata":{}},{"cell_type":"code","source":"# Keep only certain negatives\nnot_cancer = images_[(images_['cancer'] == 0) & (images_['difficult_negative_case'] == False)]\n# And all cancers\nwith_cancer = images_[images_['cancer'] == 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop_image(img, th=0):\n    mask = img > th\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\n# Doubles the image's width and height by adding black margins\ndef add_margins(img):\n    (h, w) = img.shape[:2]\n\n    # Using the diagonal is a correct way to do that\n    mx = math.ceil(2 * math.sqrt(h*h + w*w))\n    # A new square image will contain any version of the original image rotated around the center\n    new_img = np.zeros((mx,mx), np.uint8)\n    \n    hh = mx // 2 - h\n    wh = mx // 2\n    \n    # Position the original image with its bottom left corner on the center\n    new_img[hh:hh+h, wh:wh+w] = img\n    return new_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orb = cv2.ORB_create()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This method rotates the image, gets the keypoints and then transforms them back to the original images's coordinates\ndef rotate_and_reverse_keypoints(img, angle):\n    # Get the matrix for rotation around the center of the image\n    height, width = img.shape[:2] \n    center = (width/2, height/2) \n    cos = math.cos(math.radians(-angle));\n    sin = math.sin(math.radians(-angle));\n    \n    img_new = imutils.rotate(img, angle = angle)\n    \n    # Detect ORB keypoints\n    kp = orb.detect(img_new, None)\n\n    # Consider that y axis is directed down in screen coordinates\n    coords_transformed = [(k.pt[0] - center[0], center[1] - k.pt[1]) for k in kp]\n    # Rotate all the points around the center of coordinates\n    coords_transformed = [(cos * k[0] - sin * k[1], sin * k[0] + cos * k[1]) for k in coords_transformed]\n    \n    # Translate all the points back and add to the combined list\n    coords_final = [(int(k[0] + center[0]), int(center[1] - k[1])) for k in coords_transformed]\n    \n    return coords_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_points(img, points, radius=5):\n    color = (255, 0, 0)\n    thickness = 1\n    \n    # This is to convert the image from grayscale to RGB\n    img_new = np.stack((img,)*3, axis=-1)\n    \n    for coord in points:\n        #Draw circles for given points \n        point = (int(coord[0]), int(coord[1]))\n        img_new = cv2.circle(img_new, point, radius, color, thickness)\n    \n    return img_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converts the coordinates as if margins removed\n# Removes the black margins added for rotation\ndef remove_margins_shift_coords(points, img, width, height):\n    # Get the size of the image and calculate the center coordinates\n    (h, w) = img.shape[:2]\n    center = (w//2, h//2)\n    new_points = [(int(p[0])-center[0], int(p[1])-center[1]+height) for p in points]\n    new_img = img[center[1]-height:center[1], center[0]:center[0]+width]\n    return new_img,new_points","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Returns the number of keypoints for the 2D histogram\ndef count_points(points, img, width, height):\n    # Get the size of the image\n    (h, w) = img.shape[:2]\n    rows, cols = (math.ceil(h/height), math.ceil(w/width))\n    \n    # Introduce a 2D list for storing numbers\n    number_points = np.zeros((rows+1,cols+1),dtype = np.uint8)\n    for coord in points:\n        number_points[math.floor(coord[1]/height)][math.floor(coord[0]/width)] += 1\n        \n    return number_points","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw a 2D histogram on a given image using the numbers\ndef draw_histogram_old(img, points, width, height, colormin=(0, 255, 255), colormax=(255, 0, 0)):\n    # Count density of points in a given raster of rectangular cells\n    numbers = count_points(points, img, width, height)\n    \n    # If the image is a grayscale one, turn it to RGB\n    if len(img.shape) < 3:\n        img_new = np.stack((img,)*3, axis=-1)\n    else:\n        img_new = np.array(img)\n    \n    # Find the maximum and minimum in the 2D list\n    num_max = max(map(max, numbers))\n    num_min = min(map(min, numbers))\n    # Loop through all rows and columns\n    (rows, cols) = numbers.shape[:2]\n    (img_height, img_width) = img.shape[:2]\n    # Generate a histogram mask image\n    img_histo = np.zeros_like(img_new)\n    for i in range(rows):\n        for j in range(cols):\n            n = (numbers[i,j] - num_min)/num_max\n            # Interpolate color between the colormin and colormax values\n            color = ((1-n)*colormin[0] + n*colormax[0], (1-n)*colormin[1] + n*colormax[1], (1-n)*colormin[2] + n*colormax[2])\n            # Strangely it does not draw if the coordinates are outside the image area\n            img_histo = cv2.rectangle(img_histo, (j*width, i*height), (min(width*(j+1)-1, img_width-1), min(height*(i+1)-1, img_height-1)), color, -1)\n\n    #img_new = cv.bitwise_and(img_new, img_histo) # Bitwise does not work as intended\n    # Use the grayscale image as intensity for the RGM image\n    for i in range(img_height):\n        for j in range(img_width):\n            # Overflow happens if you multiply first instead of dividing to 255\n            img_new[i, j][0] = round(img_histo[i, j][0] / 255 * img_new[i, j][0])\n            img_new[i, j][1] = round(img_histo[i, j][1] / 255 * img_new[i, j][1])\n            img_new[i, j][2] = round(img_histo[i, j][2] / 255 * img_new[i, j][2])\n\n    #return img_histo\n    return img_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw a 2D histogram on a given image using the numbers\ndef draw_histogram(img, number_points):\n    # Scaling the density to [0,255]\n    min_val = np.min(number_points)\n    max_val = np.max(number_points)\n    number_points = (255.0*(number_points - min_val) / (max_val - min_val)).astype(int)\n        \n    (rows, cols) = number_points.shape[:2]\n    (img_height, img_width) = img.shape[:2]\n    \n    # Stretching the keypoint density array to the image's size\n    number_points = np.repeat(np.repeat(number_points,(img_width//cols), axis=0), (img_height//rows), axis=1)\n    \n    # If the image is a grayscale one, turn it to RGB\n    if len(img.shape) < 3:\n        img_new = np.stack((img,)*3, axis=-1)\n    else:\n        img_new = np.array(img)\n    \n    min_h = min(img_height,number_points.shape[0])\n    min_w = min(img_width,number_points.shape[1])\n    \n    # the slicing is done to align the size of matrices\n    img_new[:min_h,:min_w,0] = number_points[:min_h,:min_w]\n    img_new[:min_h,:min_w,1] = number_points[:min_h,:min_w]\n    return img_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_subimage(img, frame_idx,frame_width,frame_height):\n    (img_height, img_width) = img.shape[:2]\n    y_s = frame_idx[0]*frame_height\n    y_e = min(y_s + frame_height,img_height) # not to go beyond the image size\n    x_s = frame_idx[1]*frame_width\n    x_e = min(x_s + frame_width,img_width) # not to go beyond the image size\n    \n    return img[y_s:y_e,x_s:x_e]\n\n# Returns array of images of given size with highest numbers in histogram\ndef get_images(img, number_points, width, height, max_images, black_threshold = 0.55):\n    (rows, cols) = number_points.shape[:2]\n    (img_height, img_width) = img.shape[:2]\n    \n    # Sort the array values (to have it descending, multiplied by -1 )\n    arr_srt = np.argsort(-1*number_points,axis=None)\n    # The result is flattened (due to axis = None)\n    unidx = np.unravel_index(arr_srt,number_points.shape)\n    # Make the result as tuples - coordinates of the areas with max density\n    idx_tpls = list(zip(unidx[0],unidx[1]))\n    \n    i = 0\n    cnt = 0\n    img_arr = np.zeros((0,width,height))\n    # This will keep the index of the skipped images (due to size or black pixels)\n    skipped_images = []\n    \n    # Keep taking top max_images from the ordered list \n    while i < len(idx_tpls) and (cnt < max_images):\n        tmp = get_subimage(img, idx_tpls[i], width, height)\n    \n        # Count if the black pixels are not in majority and the shapes correspond\n        if (np.count_nonzero(tmp<10) < (width*height*black_threshold)) and (tmp.shape == img_arr.shape[1:3]):\n            img_arr = np.append(img_arr,np.expand_dims(tmp, axis=0),axis=0)\n            cnt += 1\n        else:\n            skipped_images.append(i)\n        i += 1\n\n    # If the matrix is not full, then add smaller images by padding them\n    if img_arr.shape[0] != max_images:\n        required_imgs = max_images - img_arr.shape[0]\n        i = 0\n        while i < required_imgs:\n            tmp = get_subimage(img, idx_tpls[i],width,height)\n            (h,w) = tmp.shape\n            tmp = np.pad(tmp, ((0,height-h),(0,width-w)), 'constant')\n            img_arr = np.append(img_arr,np.expand_dims(tmp, axis=0),axis=0)\n            i += 1\n    \n    return img_arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Experiments with the Konul x.\n# 312 /kaggle/input/rsna-mammogram-pngs-enhanced/png_train_images/26664/1409401831.png\n# 253 /kaggle/input/rsna-mammogram-pngs-enhanced/png_train_images/23224/1242778806.png\n# 80 /kaggle/input/rsna-mammogram-pngs-enhanced/png_train_images/13331/1949261883.png\n# 252 /kaggle/input/rsna-mammogram-pngs-enhanced/png_train_images/23101/732919056.png\n# 117 /kaggle/input/rsna-mammogram-pngs-enhanced/png_train_images/15945/890480233.png","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_id = 252 #random.randint(0,400)\n#fname = with_cancer['file_path'].iloc[sample_id]\n#fname = '/kaggle/input/rsna-mammogram-pngs-enhanced/png_train_images/23101/732919056.png'\nfname = '/kaggle/input/rsna-mammogram-pngs-enhanced/png_train_images/13331/1949261883.png'\nfname = '/kaggle/input/oncologyclinicdata/LN00030001.png'\n#82 /kaggle/input/rsna-mammogram-pngs-enhanced/png_train_images/1336/1475901500.png\nprint(sample_id,fname)\nimg_init = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\nimg_init = cv2.resize(img_init, (300,500)) # for the big images \n\nimg_cropped = crop_image(img_init, 2)\nimg_margin = add_margins(img_cropped)\n\n# Remember the size of the initial cropped image\n(height_cropped, width_cropped) = img_cropped.shape[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coords_combined = []\n\nfor angle in [15,-15,30,-30,45]:\n    coords = rotate_and_reverse_keypoints(img_margin,30)\n    coords_combined += coords\n\nimg_rm_margins, coords_combined = remove_margins_shift_coords(coords_combined, img_margin, width_cropped, height_cropped)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame_width = 50\nhistogram_numbers = count_points(coords_combined, img_rm_margins, frame_width, frame_width)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if show_visuals:\n    fig, axarr = plt.subplots(1,5, figsize=(15, 6))\n    fig.suptitle('Keypoint detection')\n    # axarr[0].set_title('Original image', loc='right')\n    # axarr[0].imshow(img_init, cmap='gray')\n    axarr[0].set_title('Cropped image', loc='right')\n    axarr[0].imshow(img_cropped, cmap='gray')\n    # axarr[2].set_title('Image with margins', loc='right')\n    # axarr[2].imshow(img_margin, cmap='gray')\n    axarr[1].set_title('All keypoints combined', loc='right')\n    axarr[1].imshow(draw_points(img_rm_margins, coords_combined, 2), cmap='gray')\n    axarr[2].set_title('Density of the keypoints', loc='right')\n    axarr[2].imshow(histogram_numbers)\n    axarr[3].set_title('High density regions', loc='right')\n    axarr[3].imshow(draw_histogram_old(img_rm_margins, coords_combined, frame_width, frame_width))\n    axarr[4].set_title('High density regions', loc='right')\n    axarr[4].imshow(draw_histogram(img_rm_margins, histogram_numbers))\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Showing the X common keypoints and the 20x20 regions around them\nif show_visuals:\n    frame_cnt = 15\n\n    a = 3\n    b = frame_cnt // a\n\n    fig, axarr = plt.subplots(b, a, constrained_layout = True, figsize=(6, 10))\n    fig.suptitle('20 common keypoints and the 20x20 regions around them')\n    img_arr  = get_images(img_rm_margins, histogram_numbers, frame_width, frame_width, frame_cnt, black_threshold = 0.8)\n\n    for i in range(b):\n        for j in range(a):\n            axarr[i, j].set_xticklabels([])\n            axarr[i, j].set_yticklabels([])\n            axarr[i, j].imshow(img_arr[i*a+j,:,:], cmap='gray')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining all in one function: receives an image and returns N frames from the interest areas\ndef get_interest_frames(img, frame_width = 20, frame_cnt = 20):\n    img_cropped = crop_image(img, 2)\n    img_margin = add_margins(img_cropped)\n    \n    (height_cropped, width_cropped) = img_cropped.shape[:2]\n    \n    coords_combined = []\n\n    for angle in [15,-15,30,-30,45]:\n        coords = rotate_and_reverse_keypoints(img_margin,30)\n        coords_combined += coords\n\n    img_rm_margins, coords_combined = remove_margins_shift_coords(coords_combined, img_margin, width_cropped, height_cropped)\n    \n    histogram_numbers = count_points(coords_combined, img_rm_margins, frame_width, frame_width)\n    img_arr  = get_images(img_rm_margins, histogram_numbers, frame_width, frame_width, frame_cnt)\n    \n    return img_arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This part is used to run the trained model on the images of Oncological Clinic\nimport tensorflow as tf\nimport keras.api._v2.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras import activations\n\nclass BC3DConvNet(tf.keras.Model):\n    def __init__(self, frame_cnt, frame_width):\n        super(BC3DConvNet, self).__init__()\n\n        self.conv1 = Conv2D(filters=frame_cnt*16, kernel_size=(3, 3), input_shape=(frame_width, frame_width, frame_cnt))\n        self.bn1 = BatchNormalization()\n        self.conv2 = Conv2D(filters=frame_cnt*16, kernel_size=(3, 3))\n        self.bn2 = BatchNormalization()\n        self.conv3 = Conv2D(filters=frame_cnt*32, kernel_size=(3, 3))\n        self.bn3 = BatchNormalization()\n        self.conv4 = Conv2D(filters=frame_cnt*64, kernel_size=(3, 3))\n        self.bn4 = BatchNormalization()\n\n        # Calculate the convolution's output size:\n        tmp = np.zeros((1, frame_width, frame_width, frame_cnt))\n        x = self.apply_conv(tmp)\n        x = Flatten()(x)\n        linear_input = x.shape[1]\n        # ------------------------------------------------------\n\n        self.fc1 = Dense(512, activation='relu')\n        self.bn5 = BatchNormalization()\n        self.fc2 = Dense(1024, activation='relu')\n        self.bn6 = BatchNormalization()\n        self.fc3 = Dense(512, activation='relu')\n        self.bn7 = BatchNormalization()\n        self.fc4 = Dense(2, activation='softmax')\n\n    def apply_conv(self, x):\n        x = activations.relu(self.conv1(x))\n        x = self.bn1(x)\n        x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n        x = activations.relu(self.conv2(x))\n        x = self.bn2(x)\n        x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n        x = activations.relu(self.conv3(x))\n        x = self.bn3(x)\n        x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n        x = activations.relu(self.conv4(x))\n        x = self.bn4(x)\n        x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n        return x\n\n    def call(self, x):\n        conv_out = self.apply_conv(x)\n        x = Flatten()(conv_out)\n        x = self.fc1(x)\n        x = self.bn5(x)\n        x = Dropout(0.4)(x)\n        x = self.fc2(x)\n        x = self.bn6(x)\n        x = Dropout(0.4)(x)\n        x = self.fc3(x)\n        x = self.bn7(x)\n        x = Dropout(0.4)(x)\n        x = self.fc4(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame_width = 50\nframe_cnt = 15\n\nif test_on_data:\n    net = BC3DConvNet(frame_cnt, frame_width)\n    net.compile(loss='binary_crossentropy', \n                optimizer=keras.optimizers.Adam(learning_rate=1e-3,weight_decay=0.1,clipnorm=1.0), \n                steps_per_execution=32,\n                metrics=['accuracy'])\n    onc_data = '/kaggle/input/oncologyclinicdata'\n    net.load_weights('/kaggle/input/bc-keras-model/BC3DConvNet')\n    for folder, subs, files in os.walk(onc_data):\n        for filename in files:\n            file_path = os.path.abspath(os.path.join(folder, filename))\n            print(file_path)\n            image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n            # the images are bigger than the trianing images\n            image = cv2.resize(image, (300,500))\n            image = get_interest_frames(image, frame_width, frame_cnt)\n            result = net.predict(tf.reshape(image,[1,frame_width,frame_width,frame_cnt]))\n            prediction = 'POSITIVE' if (np.argmax(result)==0) else 'Negative'\n            print(\"Result:\",result, prediction,\"Value:\",result[0,np.argmax(result)])\n            print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(with_cancer.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(with_cancer.shape[0], not_cancer.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example is taken from here: \n# https://stackoverflow.com/questions/56418865/writing-and-reading-lists-to-tfrecord-example\nimport tensorflow as tf\nfrom tensorflow.train import FloatList, BytesList, Int64List, Feature, Features, FeatureList, FeatureLists, Example\n\ndef _float_feature(value):\n    return Feature(float_list=FloatList(value=value))\n\ndef _int_feature(value):\n    return Feature(int64_list=Int64List(value=value))\n\ndef make_example(image, label):\n    feat_arr2d = _float_feature(image.flatten())\n\n    label_feature = _int_feature(label.flatten())\n            \n    example = Example(features=Features(feature={'image':feat_arr2d, 'label': label_feature}))\n        \n    return example.SerializeToString()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_tfrecord(folder,image_id, file_path, add_info, label):\n    try:\n        image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n        if (label == 0):\n            if random.random() > 0.5:\n                image = cv2.flip(image, 1)\n            else:\n                # randomly rotate the image in [-25,25] rangle\n                image = imutils.rotate(image, random.randint(0, 50)-25)\n        image = get_interest_frames(image, frame_width, frame_cnt)\n        \n        fname = folder + '/' + str(image_id)+'_'+str(add_info)+'.tfrecord'\n        with tf.io.TFRecordWriter(fname) as writer:\n            writer.write(make_example(image, np.eye(2)[label].astype(int)))\n    except Exception as e:\n        print(f'Error loading image from {file_path}: {str(e)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame_cnt = 15\nframe_width = 50\n\n# Folder for positive samples\ntf.io.gfile.mkdir('pos')\n# Folder for negativetive samples\ntf.io.gfile.mkdir('neg')\n\nprint('saving positives as TF records...')\nfor i in range(10):\n    for idx,rec in tqdm(with_cancer.iterrows(), total=with_cancer.shape[0]):\n        save_tfrecord('pos',rec['image_id'],rec['file_path'],i,0)\n\nprint('saving negetives as TF records...')\nfor idx,rec in tqdm(not_cancer.iterrows(), total=not_cancer.shape[0]):\n    save_tfrecord('neg',rec['image_id'],rec['file_path'],0,1)\n\nprint('Done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_images(img_arr,frame_cnt):\n    a = 5\n    b = frame_cnt // a\n\n    fig, axarr = plt.subplots(b, a, constrained_layout = True, figsize=(10, 5))\n\n    for i in range(b):\n        for j in range(a):\n            axarr[i, j].set_xticklabels([])\n            axarr[i, j].set_yticklabels([])\n            axarr[i, j].imshow(img_arr[i*a+j,:,:], cmap='gray')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation - let's check if we recorded images and labels correctly.\n\n# Read from file and show\nfilenames = ['/kaggle/working/pos/846765464_0.tfrecord']\nraw_dataset = tf.data.TFRecordDataset(filenames)\n\n# A description of the features.\nfeatures_dict = {\n    'image': tf.io.FixedLenFeature([frame_width*frame_width*frame_cnt], tf.float32),\n    'label': tf.io.FixedLenFeature([2], tf.int64),\n  }\n\ndef _parse_function(example):\n    sample_record = tf.io.parse_single_example(example,features_dict)\n    feats = sample_record['image']\n    label = sample_record['label']\n    \n    feats = tf.reshape(feats,[frame_cnt,frame_width,frame_width])\n    label = tf.reshape(label,[2,1])\n\n    return feats,label\n\nparsed_dataset = raw_dataset.map(_parse_function)\nfor img, label in parsed_dataset:\n    print(label)\n    visualize_images(img,frame_cnt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}